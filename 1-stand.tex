%!TEX root = Beschreibung_des_Vorhabens.tex
In the following, we present the state of the art and preliminary work structured according to the five main research areas relevant for this proposal.

\vspace{-0.5em}
\subsubsection*{Security in Diverse Project Environments}
\vspace{-1em}
Advanced security engineering approaches were presented in REF?. Publications such as REF show how sophisticated monitoring and security techniques can be applied to find problems and solve them. When a new attack pattern has been identified, protection and mitigation solutions are published, e.g. in the case of nnn REF. These publications provide valuable information for security experts. Ordinary developers, however, rarely read this kind of publications. Equipped with a sound background in software development, they rely on their experience and guidance by a development process. Only a few developers are also skilled security experts. For example, the European standardization institute ETSI conducts a growing number of software and telecommunication projects with an insufficient number of security experts [11]. Therefore, developers must grow awareness and skills in security. In previous common work, the applicants have developed an approach to identify security-related requirements in natural-language specifications [16]. When there is a flow of similar projects and specifications, Naïve Bayessian classifiers, SVP, or other machine learning mechanisms can be applied. Knauss et al. show that supervised learning from other projects is possible, but does not reach the precision and recall that can be achieved in a k-fold training and test set from a documentation from the same project and context [16]. In short, the closer a training set is to the examined body of text, the better performance can be expected.

During the two phases of the DFG Priority Project 1593, the applicants shifted their attention from a constant flow of similar inputs (specifications) to the situation in a long-living software project. Every active project tends to degrade due to ad-hoc fixes and extensions. In particular, security can suffer when sensitive data is added to a software package, or when attackers find new ways of threatening the security of the system. Even an unchanged system can degrade in terms of security due to outdated assumptions about its environments [9] (see the example above). A single long-living project can hardly provide a sufficiently large training set of security issues for many machine learning classifiers. Therefore, we adopted a security ontology and populated it with instances from the application domain. Using semantic frames, potential security problems could be identified heuristically – not statistically, as with machine learning classifiers REF. 

While our own previous work explored possibilities for machine learning, a learning loop assisted in gaining additional training data from past projects. Our work was a stand-alone contribution to a specific step in development. It was not embedded in a development process, did not build on well-known concepts such as quality models or traces, and did not address human individual learning as a key concept of merging secure development, problem analysis, and learning. 

There are several development processes that focus on security, such as Microsoft SDL REF. Neither end of the spectrum fits the needs of an ordinary. A main challenge for many software projects is that the exact level of importance of the various security requirements is not known in advance and can shift as the process develops. Consideration of security issues must match and follow growing demand and growing awareness. Explicit feedback to using artifacts and techniques is an essential contribution to making security both traceable and understandable in an ordinary project. Security concerns must be embedded and integrated, and the effort devoted to security must be justified with respect to priorities and concrete constraints (e.g. vulnerabilities, business cases, assets). Traceability benefits from a predefined process and weaves security into it: From requirements over priorities to managing techniques, heuristics, and monitoring data.  We plan to use standard/waterfall and agile procedures as representatives of that scope and investigate how and to which degree the intended methodology can serve those representations.

There are process descriptions like Microsoft SDL. They list adequate security techniques for each step of a waterfall process. However, they do not support the hand-over between different steps with respect to concrete, given security issues. Microsoft SDL focusses entirely on security, ignoring any tradeoffs with other quality attributes. The process connects existing security activities in a meaningful way. Many code-level hints are provided. TraceSEC will make active use of security traces, e.g. by using them to generate tutorials for developers, referring to the concrete artifacts occurring in their project. They are put in perspective by the relationships induced by traces. Checklists and even tutorial videos will be generated to support development in a focused way. As a by-product, developers receive contextualized material for constant learning.


\vspace{-0.5em}
\subsubsection*{Quality and Knowledge Model}
\vspace{-1em}
In many software projects, quality models adopt existing general models, such as ISO 25 010 or Boehm’s so-called Quality Tree [4] as their quality model. This type of general models provides a structured overview of well-known software quality aspects (e.g. usability, portability) and their relationships. Quality models are often used for specifying which selection of quality aspects are particularly important for a given software. In more advanced quality models [25], general terms (e.g., security, usability) are refined to what those aspects mean in the particular project. For example, security could be refined into nnnVerschiedeneSecurityUseCases. 

Schneider [25] describes a three-layer format of quality models; the top layer contains a selection of generic quality attributes. They are selected and prioritized but may still be interpreted differently between customers and stakeholders at this point. By refining those generic aspects, the second layer of the quality model contains concrete interpretations and instances of selected quality aspects, such as the design considerations mentioned in the running example. On the bottom layer, each of the concrete quality aspects is further detailed. There are heuristics and metrics for measuring the extent of meeting the respective quality requirement. A full-fletched quality model starts from informally presented selections of quality aspects. Each selected aspect is refined into cases where the quality is of particular importance. This concrete layer provides concrete specifics. On the bottom layer, specific quality attributes can be formally measured, counted, or assessed using explicit metrics. 

Wagner et al. [36] describe their Quamoco approach. They claim there is no seamless integration of abstract generic terms and security measurement.  They propose richer quality models for security, similar to the layers described in [25]. The report on spending over three years’ time on bridging the gap. They suggest using GQM [3] [32] to guide the refinement process and wander whether such an endeavor is feasible in practical projects. We consider it essential to reduce effort and increasing value by recording and reusing lessons learned.

In TraceSEC, we use extended Quality Models. They are not limited to security but may contain other quality aspects. For example, usability is often competing with security: High standards for passwords, for example, may raise the bar for memorizing them. It is therefore important to prioritize aspects of usability against aspects of security.

Extended quality models contain the three levels described above and may be enriched by further intermediate results on the way from informal priorities to formal monitoring and ensuring of security properties REFkoblenz. With so much information about security centralized in one artifact, extended quality models are a rich source of examples and recommendations in similar cases. In TraceSEC, quality models are developed into a repository for storing and maintaining knowledge; knowledge about everything related to security in that project [26]. There are two dimensions in the quality model: Along the dimension of security refinement and along the dimension of development progress. Traces weave both dimensions together.

While quality models can collect intermediate results, they cannot express dependencies and temporal relationships between the different parts of the quality model. Extended quality models have a dual nature. Since we strive for extracting reusable parts from the quality model, a knowledge perspective can help: It emphasizes the need to treat parts of the quality model as pieces of knowledge. They can be used and reused in various forms (development, problem analysis, learning). Treating an extended quality model as a knowledge representation highlights its value as an asset independent of a particular project. Organizational learning relies on externalizing (e.g., by provoking breakdowns, according to Schön [20] and reusing knowledge elsewhere, and for a different purpose. 

\vspace{-0.5em}
\subsubsection*{Security-related Activities and Analyses}
\vspace{-1em}
Automated security checks and metrics are a state-of-the-art approach for continuously estimating the security standards of software systems during development in terms of bugs, vulnerabilities, and code-smells. Platforms like SonarQuble are closely integrated into continuous integration approaches and are executed without user interaction. While this allows deep insights into the system, there are usually very many warnings or issues with only a low relevance to the project.

According to the OWASP Top 10 application security risks, the most dangerous threat is the injection of untrusted data. Accordingly, a lot of works analyze calls to critical APIs, like SQL queries, and check if external data is forwarded to the APIs unfiltered or check parameter ranges. One special field for this kind of analysis are misuses of crypto APIs. Many crypto APIs are very complicated and often used in a non-secure way. As such analyses usually are very locally they might produce a lot of warnings for not that dangerous calls, as no information about the structure of the system and the requirements is available.

Another OWASP Top 10 threat is the exposure of sensitive data. Instances of this threat are usually detected using secure data flow or taint analyses. Critical data is only allowed to flow from secure sources to secure sinks. There are a lot of tools providing such analyses like FlowDroid or SuSi. Unfortunately, they have to rely on manually created or learned libraries of critical sources and sinks, making the results not perfectly suitable to the system.

All of the security issues discussed above might already have been reported as concrete vulnerabilities (CVE) and general weaknesses (CWE) have been extracted from those weaknesses. Specific implementation structures might be directly connected to well-known CWEs threatening the systems security: E.g. throwing Runtime-Exceptions threatens availability. Many tools like Checkstyle, Findbugs, SonarQube, and others check for this reason for such simple patterns and report them. Especially this very simple patterns are violated very often leading to very long reports that are usually not prioritized.

Even if the own product itself does not contain any critical security issues, its security could be at stake due to used libraries having security issues. Such kinds of issues might arise from the use of old (and maybe already known as insecure) libraries. Accordingly, there is a lot of work on analyzing the dependencies of a project and checking if they are known to have unfixed security issues.

Last but not least various security metrics have been developed for estimating parts of the system that are of high importance. Relations between security-relevant and non-security-relevant elements, might allow a prediction on the suitability of making security critical mistakes. Unfortunately, this kind of metrics usually need additional information, like the critical attributes of the system, that usually is not part of the code but is a part of the quality model discussed before.

While the approaches discussed above only focus on the implementation approaches like STRIDE, UMLsec, SecDFDs, or SecBPMN can be used to plan required security mechanisms at requirements engineering or at design-time. Their aim is to support security from design-time and to provide mechanisms for identification and planning of required security mechanisms. For this purpose, UMLsec extends UML with various security properties for the specification of security requirements. The specified security requirements can be checked for conciseness and completeness according to different security policies provided by UMLsec.

\vspace{-0.5em}
\subsubsection*{Elicitation and Interpretation of Traces}
\vspace{-1em}
During the development of software systems, developers leave traces. Their use of shared development environments like issue trackers, source code repositories, or build systems can be automatically tracked. These systems usually log developer activities and their exact date and time when an issue was created, changed, or commented, when code has been committed, or when a system build has been configured or executed. This information is stored as meta data on the corresponding content or logged in a log system. It can be collected as raw traces to be analyzed in real time or post-factum.

Raw traces can be helpful for measuring the product \cite{ProductMeasures} or process quality \cite{ProcessMeasures}, but they can also be interpreted to understand and evaluate developer activities  in context \cite{TraceInterpretation} and to relate development artifacts to each other. The latter creates \emph{trace links} between artifacts, which allow multiple artifacts to be analyzed together. By combining the analysis of traces (activities upon artifacts) and trace links (relations between artifacts), development activities can be understood on a higher level.

In our previous work, we generate traces between software development artifacts, e.g., security requirements and system design models \cite{Houmb2009}, or design models and code \cite{Peldszus2019}.

Recording large quantities of traces may pose a challenge: First, traces need to be maintained, which adds another burden to the development process \cite{TraceMaintenance}. Second, large trace sets slow down the analysis considerably. Therefore, a balance has to be found between collecting all possible traces and collecting too few traces for the analysis.

In TraceSEC we collect developer activities from a variety of sources. The raw traces will be filtered, semantically grouped for interpretation and interconnected to sequences of related activities for identifying security-related interconnections between artifacts, activities or the lack thereof.

\vspace{-0.5em}
\subsubsection*{Innovative Reuse of Security Knowledge}
\vspace{-1em}
There is a wide spectrum of quality aspects, sub-aspects, and respective representations in one quality model of a specific software project. Aspects start from overall priorities and refine into security requirements, heuristics, and mechanisms. This multi-level system of formal and informal, automatable and decision-related information is currently spread over many artifacts. We envision to integrate the pieces in an extended quality model, capture relationships through fine-grained traces, and treat the result as a security knowledge representation. Weaving such a complex web of interrelated items must be supported automatically, and can be justified by using and reusing it for several purposes.

In earlier work, Schneider et al. studied systematic learning from experience in companies [22] [28] [10]. The concept of an experience factory by Basili [2] was applied and adapted to suit the needs of software companies [30]. There were technical findings concerning the central repository for learning from experience [30] [23]. Reflections on the use and benefit of experiential learning in software development, key lessons learned included [23]: An efficient structure for storing new experiences is essential for a large-scale sharing of experiences in an organization. However, identifying content such as valuable observations and tacit knowledge [19] is even more critical for success. Usually, experts and experienced staff are not aware of what others need. Therefore, mechanisms to support identifying that knowledge are typically socio-technical, including technical mechanisms for supporting awareness and externalization [18] [1] [15]. 

In TraceSEC, there are two analogous challenges: (1) Identification of a piece of information that could be relevant for others. The above-mentioned challenge will be addressed by trying to use quality model entries that are provided anyway, and tie them together with fine-grained traces of activities. The latter should be collected as a by-product. Explicit comments by the experts will only be requested if there is a pattern or heuristic indicating the opportunity to capture tacit knowledge. Thus, our methods and research approach is informed by the lessons learned in the above-mentioned experiential learning. However, TraceSEC addresses security in-depth, whereas SEC explored experiential learning on a far broader and more superficial level.

The concept of a by-product is important for TraceSEC. Schneider [24] proposed a scheme of recording events in which important knowledge surfaced while some activity was carried out. Originally, the demonstration of an innovative prototype was used: Whenever a prototype is demonstrated and explained, the knowledge engrained in that prototype is complemented by the requirements and knowledge of its creator and demonstrator. Schneider [24], therefore, proposed several technical and methodological steps to record that demonstrations as run-time trace and as screen-capture video with the explanations on the audio track. Almost no additional effort is necessary, and the resulting web of traces and explanations can be exploited in several ways [24]. Other researchers have used the by-product paradigm to capture annotations in pdf documents for rationale [1] [15]. Noch ein Beispiel, SecVol?

When knowledge and learning is combined with sophisticated technical tasks, e.g. in software and security engineering, both technical and human aspects should be considered together. The optimal benefit often depends on reducing participant effort to the minimum by using more powerful support. The strength of this socio-technical situation comes from applying rather inexpensive computational power to rather informal and valuable sources of human skills that can still not be substituted by machines – partly because they refer to human experience, decisions, and values. Exploiting rich knowledge repositories for supporting humans, as in REF?, is the vision we follow.

\vspace{-0.5em}
\subsubsection*{Preliminary work}
\label{sec:preliminary_work}
\vspace{-1em}
